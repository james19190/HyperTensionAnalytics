{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SByTDY_pVr1r",
        "outputId": "0b65d2ef-c064-4e4b-8dfd-ff5a27264118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, roc_curve, auc\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "Fslje4kUXsjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = './filtered_merged_encoded_data.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "7gvI3CnAYI-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFdSYWu0YNYu",
        "outputId": "aabe4abd-09ba-4364-f353-4eb63f04c47d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24867 entries, 0 to 24866\n",
            "Data columns (total 24 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   HY_YN             24867 non-null  int64  \n",
            " 1   AGE               24867 non-null  float64\n",
            " 2   BLDS              24867 non-null  float64\n",
            " 3   TOT_CHOLE         24867 non-null  float64\n",
            " 4   GAMMA_GTP         24867 non-null  float64\n",
            " 5   SCR_CNT           24867 non-null  float64\n",
            " 6   T_IN_LOS          24867 non-null  float64\n",
            " 7   T_OUT_LOS         24867 non-null  float64\n",
            " 8   BMI               24867 non-null  float64\n",
            " 9   Liver_Enzyme_Avg  24867 non-null  float64\n",
            " 10  MAP               24867 non-null  float64\n",
            " 11  SEX_2             24867 non-null  bool   \n",
            " 12  SMK_STAT_1        24867 non-null  bool   \n",
            " 13  SMK_STAT_2        24867 non-null  bool   \n",
            " 14  SMK_STAT_3        24867 non-null  bool   \n",
            " 15  DRNK_HABIT_2      24867 non-null  bool   \n",
            " 16  DRNK_HABIT_3      24867 non-null  bool   \n",
            " 17  DRNK_HABIT_4      24867 non-null  bool   \n",
            " 18  DRNK_HABIT_5      24867 non-null  bool   \n",
            " 19  EXER_2            24867 non-null  bool   \n",
            " 20  EXER_3            24867 non-null  bool   \n",
            " 21  EXER_5            24867 non-null  bool   \n",
            " 22  H_APOP_YN_1       24867 non-null  bool   \n",
            " 23  H_HDISE_YN_1      24867 non-null  bool   \n",
            "dtypes: bool(13), float64(10), int64(1)\n",
            "memory usage: 2.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *정규화*\n",
        "standard vs minmax vs robust"
      ],
      "metadata": {
        "id": "AphpFKo8Ze4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#수치형변수들의 분포 확인\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "columns = ['AGE', 'BLDS', 'TOT_CHOLE', 'GAMMA_GTP', 'SCR_CNT', 'T_IN_LOS', 'T_OUT_LOS', 'BMI', 'Liver_Enzyme_Avg', 'MAP']\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "for i, col in enumerate(columns, 1):\n",
        "    plt.subplot(5, 2, i)\n",
        "    plt.hist(df[col], bins=30, alpha=0.7, edgecolor='black')\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wZabAjcWaB05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "# MinMaxScaler 적용 변수들\n",
        "minmax_columns = ['AGE', 'SCR_CNT']\n",
        "# StandardScaler 적용 변수들\n",
        "standard_columns = ['BLDS', 'TOT_CHOLE', 'BMI', 'MAP']\n",
        "# RobustScaler 적용 변수들\n",
        "robust_columns = ['GAMMA_GTP', 'T_IN_LOS', 'T_OUT_LOS', 'Liver_Enzyme_Avg']\n",
        "\n",
        "# MinMaxScaler 적용\n",
        "scaler_minmax = MinMaxScaler()\n",
        "df[minmax_columns] = scaler_minmax.fit_transform(df[minmax_columns])\n",
        "\n",
        "# StandardScaler 적용\n",
        "scaler_standard = StandardScaler()\n",
        "df[standard_columns] = scaler_standard.fit_transform(df[standard_columns])\n",
        "\n",
        "# RobustScaler 적용\n",
        "scaler_robust = RobustScaler()\n",
        "df[robust_columns] = scaler_robust.fit_transform(df[robust_columns])"
      ],
      "metadata": {
        "id": "xNcnaxcvZAm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "BcvlAdZUQDp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GAN"
      ],
      "metadata": {
        "id": "PZdfhKukklko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['HY_YN'], random_state=42)\n",
        "\n",
        "minority_data = train_df[train_df['HY_YN'] == 1].drop(columns=['HY_YN']).values.astype(float)\n",
        "minority_labels = np.ones((len(minority_data), 1))\n",
        "\n",
        "minority_data_tensor = torch.tensor(minority_data, dtype=torch.float32)\n",
        "minority_labels_tensor = torch.tensor(minority_labels, dtype=torch.float32)\n",
        "\n",
        "minority_loader = DataLoader(TensorDataset(minority_data_tensor, minority_labels_tensor), batch_size=32, shuffle=True)\n",
        "\n",
        "# 모델 정의\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim + 1, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        input = torch.cat((noise, labels), dim=1)\n",
        "        return self.model(input)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim + 1, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, data, labels):\n",
        "        input = torch.cat((data, labels), dim=1)\n",
        "        return self.model(input)\n",
        "\n",
        "latent_dim = 50\n",
        "num_features = train_df.shape[1] - 1\n",
        "num_samples_needed = (len(train_df[train_df['HY_YN'] == 0]) // 2) - len(train_df[train_df['HY_YN'] == 1])  # 정확한 2:1 비율로 샘플 수 계산\n",
        "\n",
        "generator = Generator(latent_dim, num_features)\n",
        "discriminator = Discriminator(num_features)\n",
        "\n",
        "# 손실 함수와 옵티마이저 설정\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
        "\n",
        "# GAN 학습 루프\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    for real_data, labels in minority_loader:\n",
        "        # Discriminator 학습\n",
        "        optimizer_D.zero_grad()\n",
        "        real_labels = torch.ones(real_data.size(0), 1)\n",
        "        real_loss = criterion(discriminator(real_data, labels), real_labels)\n",
        "\n",
        "        noise = torch.randn(real_data.size(0), latent_dim)\n",
        "        fake_data = generator(noise, labels)\n",
        "        fake_labels = torch.zeros(real_data.size(0), 1)\n",
        "        fake_loss = criterion(discriminator(fake_data.detach(), labels), fake_labels)\n",
        "\n",
        "        d_loss = real_loss + fake_loss\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Generator 학습\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss = criterion(discriminator(fake_data, labels), real_labels)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch [{epoch}/{num_epochs}]  D Loss: {d_loss.item():.4f}  G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "\n",
        "generated_samples = []\n",
        "while len(generated_samples) < num_samples_needed:\n",
        "    noise = torch.randn(1, latent_dim)\n",
        "    label = torch.ones((1, 1))\n",
        "    fake_sample = generator(noise, label).detach().numpy()\n",
        "    generated_samples.append(fake_sample)\n",
        "\n",
        "\n",
        "generated_samples = np.vstack(generated_samples)\n",
        "generated_df = pd.DataFrame(generated_samples, columns=train_df.columns.drop('HY_YN'))\n",
        "generated_df['HY_YN'] = 1\n",
        "\n",
        "# 최종 train 데이터셋 결합\n",
        "train_balanced = pd.concat([train_df, generated_df], ignore_index=True)\n",
        "print(f\"최종 train 데이터셋 클래스 분포: \\n{train_balanced['HY_YN'].value_counts()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG-iLByGSCAP",
        "outputId": "7c2e4619-f786-4a3d-93dc-e590257666ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/1000]  D Loss: 1.2355  G Loss: 0.6199\n",
            "Epoch [100/1000]  D Loss: 1.3142  G Loss: 0.7132\n",
            "Epoch [200/1000]  D Loss: 1.2942  G Loss: 0.7545\n",
            "Epoch [300/1000]  D Loss: 1.4305  G Loss: 0.7021\n",
            "Epoch [400/1000]  D Loss: 1.3217  G Loss: 0.8086\n",
            "Epoch [500/1000]  D Loss: 1.3829  G Loss: 0.7260\n",
            "Epoch [600/1000]  D Loss: 1.2608  G Loss: 0.7528\n",
            "Epoch [700/1000]  D Loss: 1.2389  G Loss: 0.7907\n",
            "Epoch [800/1000]  D Loss: 0.9296  G Loss: 1.0388\n",
            "Epoch [900/1000]  D Loss: 1.0207  G Loss: 1.0773\n",
            "최종 train 데이터셋 클래스 분포: \n",
            "HY_YN\n",
            "0    18364\n",
            "1     9182\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}